{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from random import shuffle\n",
    "print(torch.cuda.is_available())\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=(3,3))\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=(3,5))\n",
    "        self.fc1 = nn.Linear(12544, 128)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, (2,4))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, (2,4))\n",
    "        x = x.view(-1, 12544)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return torch.sigmoid(x)\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 12544)\n",
    "        self.deconv1 = nn.ConvTranspose2d(64, 64, kernel_size=(5,9))\n",
    "        self.deconv2 = nn.ConvTranspose2d(64, 1, kernel_size=(3,3), padding=1)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = x.view(-1, 64, 14, 14)\n",
    "        x = F.interpolate(x, scale_factor=(2,4))\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        x = F.interpolate(x, scale_factor=(2,4))\n",
    "        x = F.sigmoid(self.deconv2(x))\n",
    "        return x\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=(3,3))\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=(3,5))\n",
    "        self.fc1 = nn.Linear(12544, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "        self.dropout = nn.Dropout(.2)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 64, 256)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.max_pool2d(x, (2,4))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.max_pool2d(x, (2,4))\n",
    "        x = x.view(-1, 12544)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongDataTrain(Dataset):\n",
    "    def __init__(self, path):\n",
    "        with np.load(path) as f:\n",
    "            data = f['X']\n",
    "            labels = list(f['T'])\n",
    "        data = np.asarray([[i[:2560] for i in j] for j in data])\n",
    "        seed = data[:, :, :256]\n",
    "        for i in range(1, 10):\n",
    "            seed = np.append(seed, data[:, :, i*256:(i+1)*256], axis=0)\n",
    "        self.normalizer = np.max(seed)\n",
    "        self.data = torch.tensor(seed, dtype=torch.float) / np.max(seed)\n",
    "        self.data = self.data.view(-1, 1, 64, 256)\n",
    "        labels = labels * 10 #expand dimensions accordingly\n",
    "        label_set = set(labels)\n",
    "        mapping = {}\n",
    "        for count, i in enumerate(label_set):\n",
    "            mapping[i] = count\n",
    "        targets = np.zeros(len(labels))\n",
    "        for i in range(len(targets)):\n",
    "            targets[i] = mapping[labels[i]]\n",
    "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
    "        \n",
    "        indices = np.random.choice(np.arange(len(self.targets)), len(self.targets), replace=False)\n",
    "        self.targets = self.targets[indices[int(.1 * len(self.targets)):]]\n",
    "        self.data = self.data[indices[int(.1 * len(self.data)):]]\n",
    "    def __getitem__(self, index):\n",
    "        return (self.data[index], self.targets[index])\n",
    "    def __len__(self):\n",
    "        return (len(self.targets))\n",
    "class SongDataTest(Dataset):\n",
    "    def __init__(self, path):\n",
    "        with np.load(path) as f:\n",
    "            data = f['X']\n",
    "            labels = list(f['T'])\n",
    "        data = np.asarray([[i[:2560] for i in j] for j in data])\n",
    "        seed = data[:, :, :256]\n",
    "        for i in range(1, 10):\n",
    "            seed = np.append(seed, data[:, :, i*256:(i+1)*256], axis=0)\n",
    "        self.normalizer = np.max(seed)\n",
    "        self.data = torch.tensor(seed, dtype=torch.float) / np.max(seed)\n",
    "        self.data = self.data.view(-1, 1, 64, 256)\n",
    "        labels = labels * 10 #expand dimensions accordingly\n",
    "        label_set = set(labels)\n",
    "        mapping = {}\n",
    "        for count, i in enumerate(label_set):\n",
    "            mapping[i] = count\n",
    "        targets = np.zeros(len(labels))\n",
    "        for i in range(len(targets)):\n",
    "            targets[i] = mapping[labels[i]]\n",
    "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
    "        \n",
    "        indices = np.random.choice(np.arange(len(self.targets)), len(self.targets), replace=False)\n",
    "        self.targets = self.targets[indices[:int(.1 * len(self.targets))]]\n",
    "        self.data = self.data[indices[:int(.1 * len(self.data))]]\n",
    "    def __getitem__(self, index):\n",
    "        return (self.data[index], self.targets[index])\n",
    "    def __len__(self):\n",
    "        return (len(self.targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = SongDataTrain(\"../audio_sr_label.npz\") #initializes our dataset\n",
    "dataset_test = SongDataTest(\"../audio_sr_label.npz\") #initializes our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = .1\n",
    "dataloader_train = DataLoader(dataset_train, shuffle=True, batch_size=128)\n",
    "dataloader_test = DataLoader(dataset_test, shuffle=True, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()\n",
    "decoder = Decoder()\n",
    "classifier = Model()\n",
    "classifier.load_state_dict(torch.load(\"./cnn_model\", map_location='cpu'))\n",
    "classifier.eval()\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-3)\n",
    "epochs = 1\n",
    "alpha = .95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanatdeshpande/anaconda3/envs/DL/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "valid_loss = []\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "zero = torch.zeros((128,128))\n",
    "for i in range(zero.shape[0]):\n",
    "    for j in range(zero.shape[1]):\n",
    "        if i < j:\n",
    "            zero[i][j] = 1\n",
    "for epoch in range(epochs):\n",
    "    tr = 0.0\n",
    "    for i, (x, _label) in enumerate(dataloader_train):\n",
    "        optimizer.zero_grad()\n",
    "        z = encoder(x)\n",
    "        y = decoder(z)\n",
    "        \n",
    "        matrix = torch.pow(z.unsqueeze(0).repeat(z.shape[0], 1, 1)\n",
    "                           - z.unsqueeze(1), 2).sum(2)\n",
    "        matrix = matrix * zero\n",
    "        partition = np.argpartition(torch.argmax(matrix, dim=0), 12)\n",
    "        in_partition = partition[-12:]\n",
    "        out_partition = partition[:-12]\n",
    "        like_loss = torch.zeros(1)\n",
    "        unlike_loss = torch.zeros(1)\n",
    "        for a in range(z.shape[0]):\n",
    "            for b in range(z.shape[0]):\n",
    "                z_a = z[a] / torch.norm(z[a])\n",
    "                z_b = z[b] / torch.norm(z[b])\n",
    "                if torch.tensor(a) in in_partition and torch.tensor(b) in in_partition:\n",
    "                    like_loss += F.cosine_similarity(z_a.unsqueeze(0), z_b.unsqueeze(0))\n",
    "                else:\n",
    "                    unlike_loss += torch.abs(\n",
    "                                    F.cosine_similarity(z_a.unsqueeze(0), z_b.unsqueeze(0)))\n",
    "        \n",
    "        criterion = nn.BCELoss()\n",
    "        like_normalizer = (1 - alpha) / 12\n",
    "        unlike_normalizer = 1 / (128**2 - 12)\n",
    "        loss = unlike_normalizer * unlike_loss - like_normalizer * like_loss + 10 * criterion(y, x)\n",
    "        loss.backward()\n",
    "        tr += float(loss.data)\n",
    "        optimizer.step()\n",
    "        break\n",
    "        if i % 1 == 0:\n",
    "            print(tr/100)\n",
    "            tr = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "346.64140625\n"
     ]
    }
   ],
   "source": [
    "centroids = torch.rand((10, 128))\n",
    "s = None\n",
    "like_normalizer, unlike_normalizer = 0.0,0.0\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-3)\n",
    "for epoch in range(epochs):\n",
    "    tr = 0.0\n",
    "    for i, (x, _label) in enumerate(dataloader_train):\n",
    "        optimizer.zero_grad()\n",
    "        z = encoder(x)\n",
    "        y = decoder(z)\n",
    "        if i % 50 == 0:\n",
    "            z = z / torch.norm(z, dim=1).unsqueeze(1) #normalizes z's\n",
    "            z = z + torch.rand(128, 128) / 10 #regularization\n",
    "\n",
    "            matrix = torch.pow(z.unsqueeze(0).repeat(centroids.shape[0], 1, 1) \n",
    "                               - centroids.unsqueeze(1), 2).sum(2)\n",
    "            indices = torch.argmax(matrix, dim=0)\n",
    "            print(indices)\n",
    "            s = torch.zeros((10,128)).scatter_(0, indices.unsqueeze(0), torch.ones(128).unsqueeze(0))\n",
    "            centroids = torch.matmul(centroids, z) * s\n",
    "            \n",
    "            partition = np.argpartition(torch.argmax(matrix, dim=0), 12)\n",
    "            in_partition = partition[-12:]\n",
    "            out_partition = partition[:-12]\n",
    "            like_loss = torch.zeros(1)\n",
    "            unlike_loss = torch.zeros(1)\n",
    "            for a in range(z.shape[0]):\n",
    "                for b in range(z.shape[0]):\n",
    "                    z_a = z[a] / torch.norm(z[a])\n",
    "                    z_b = z[b] / torch.norm(z[b])\n",
    "                    if torch.tensor(a) in in_partition and torch.tensor(b) in in_partition:\n",
    "                        like_loss += F.cosine_similarity(z_a.unsqueeze(0), z_b.unsqueeze(0))\n",
    "                    else:\n",
    "                        unlike_loss += torch.abs(\n",
    "                                        F.cosine_similarity(z_a.unsqueeze(0), z_b.unsqueeze(0)))\n",
    "        \n",
    "        like_normalizer = (1 - alpha) / 12\n",
    "        unlike_normalizer = 1 / (128**2 - 12)\n",
    "        \n",
    "        loss = matrix.sum() - 4 * criterion(y, x)  - unlike_normalizer * unlike_loss - like_normalizer * like_loss\n",
    "        loss.backward(retain_graph=True)\n",
    "        tr += float(loss.data)\n",
    "        optimizer.step()\n",
    "        if i % 50 == 0:\n",
    "            print(tr/100)\n",
    "            tr = 0.0\n",
    "            torch.save(encoder.state_dict(), \"encoder\")\n",
    "            torch.save(decoder.state_dict(), \"decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
