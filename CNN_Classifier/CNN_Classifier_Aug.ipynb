{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=(3,3))\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=(3,5))\n",
    "        self.fc1 = nn.Linear(12544, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "        self.dropout = nn.Dropout(.2)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 64, 256)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.max_pool2d(x, (2,4))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.max_pool2d(x, (2,4))\n",
    "        x = x.view(-1, 12544)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess(model, songs, labels):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(50):\n",
    "        index = np.random.randint(len(labels))\n",
    "        pred = model(songs[index])\n",
    "        if torch.argmax(pred, dim=1) == labels[index]:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    model.train()\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(\"../audio_sr_label.npz\") as f:\n",
    "    data = f['X']\n",
    "    labels = list(f['T'])\n",
    "data = np.asarray([[i[:2560] for i in j] for j in data])\n",
    "seed = data[:, :, :256]\n",
    "for i in range(1, 10):\n",
    "    seed = np.append(seed, data[:, :, i*256:(i+1)*256], axis=0)\n",
    "data = torch.tensor(seed, dtype=torch.float)\n",
    "labels = labels * 10 #expand dimensions accordingly\n",
    "label_set = set(labels)\n",
    "mapping = {}\n",
    "for count, i in enumerate(label_set):\n",
    "    mapping[i] = count\n",
    "targets = np.zeros(len(labels))\n",
    "for i in range(len(targets)):\n",
    "    targets[i] = mapping[labels[i]]\n",
    "targets = torch.tensor(targets, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = targets[int(len(targets)/10):]\n",
    "labels_test = targets[:int(len(targets)/10)]\n",
    "data_train = data[int(len(data)/10):]\n",
    "data_test = data[:int(len(data)/10)]\n",
    "#double check ^^ above stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "optimizer = optim.Adam(list(model.parameters()), lr=1e-5)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    data_train.cuda()\n",
    "    data_test.cuda()\n",
    "    labels_train.cuda()\n",
    "    labels_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(1.4927, grad_fn=<NllLossBackward>)\n",
      "0.2 0.22\n",
      "loss:  tensor(2.2587, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(1.6263, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.2089, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.3126, grad_fn=<NllLossBackward>)\n",
      "0.2 0.3\n",
      "loss:  tensor(2.3232, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.3250, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.2593, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.3149, grad_fn=<NllLossBackward>)\n",
      "0.12 0.26\n",
      "loss:  tensor(2.3229, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.2507, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.3147, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.3110, grad_fn=<NllLossBackward>)\n",
      "0.14 0.22\n",
      "loss:  tensor(2.3178, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.3235, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.2972, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.2490, grad_fn=<NllLossBackward>)\n",
      "0.14 0.22\n",
      "loss:  tensor(1.8165, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.4256, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.3701, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.1616, grad_fn=<NllLossBackward>)\n",
      "0.2 0.08\n",
      "loss:  tensor(1.9286, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.3032, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.3060, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.2397, grad_fn=<NllLossBackward>)\n",
      "0.16 0.2\n",
      "loss:  tensor(2.3173, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.4214, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.3279, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(1.9981, grad_fn=<NllLossBackward>)\n",
      "0.18 0.16\n",
      "loss:  tensor(2.3300, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.1692, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.3294, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.3197, grad_fn=<NllLossBackward>)\n",
      "0.18 0.3\n",
      "loss:  tensor(2.3846, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.3018, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.3330, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.3139, grad_fn=<NllLossBackward>)\n",
      "0.26 0.2\n",
      "loss:  tensor(2.3410, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.3139, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.3145, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.3537, grad_fn=<NllLossBackward>)\n",
      "0.2 0.08\n",
      "loss:  tensor(2.3345, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.3174, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.3402, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.4050, grad_fn=<NllLossBackward>)\n",
      "0.2 0.18\n",
      "loss:  tensor(2.3430, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.3135, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.2632, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.3101, grad_fn=<NllLossBackward>)\n",
      "0.16 0.2\n",
      "loss:  tensor(2.3441, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.3196, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.3225, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.3729, grad_fn=<NllLossBackward>)\n",
      "0.18 0.18\n",
      "loss:  tensor(2.2303, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.3163, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.3132, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.3121, grad_fn=<NllLossBackward>)\n",
      "0.18 0.16\n",
      "loss:  tensor(1.5106, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.2306, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(1.4632, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.3124, grad_fn=<NllLossBackward>)\n",
      "0.16 0.14\n",
      "loss:  tensor(2.3736, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(1.9442, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.3201, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(1.5986, grad_fn=<NllLossBackward>)\n",
      "0.14 0.16\n",
      "loss:  tensor(2.3994, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(1.5184, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(1.7457, grad_fn=<NllLossBackward>)\n",
      "loss:  tensor(2.3139, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "training_acc = []\n",
    "validation_acc = []\n",
    "model.train()\n",
    "for epoch in range(1):\n",
    "    for i in range(len(labels_train)):\n",
    "        index = np.random.randint(len(labels_train))\n",
    "        optimizer.zero_grad()\n",
    "        target = labels_train[index]\n",
    "        prediction = model(data_train[index])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(prediction, target.unsqueeze(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 5 == 0:\n",
    "            print(\"loss: \", loss)\n",
    "            if i % 20 == 0:\n",
    "                training_acc.append(assess(model, data_train, labels_train))\n",
    "                validation_acc.append(assess(model, data_test, labels_test))\n",
    "                print(training_acc[-1], validation_acc[-1])\n",
    "                if i % 100 == 0:\n",
    "                    with open(\"training_acc\", \"ab\") as f:\n",
    "                        np.asarray(training_acc).tofile(f)\n",
    "                    with open(\"validation_acc\", \"ab\") as f:\n",
    "                        np.asarray(validation_acc).tofile(f)\n",
    "                    training_acc = []\n",
    "                    validation_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
